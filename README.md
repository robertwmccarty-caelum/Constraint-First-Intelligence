# Constraint-First-Intelligence
Alignment as Geometric Projection

````markdown name=README.md
# Robert McCarty (robertwmccarty-caelum) ğŸ‘‹

> Constraint-first researcher Â· Systems-minded ML engineer Â· Open-source builder

I build **constraint-first intelligence systems** â€” architectures where capability is learned, but **continuity, safety, and identity are enforced**.  
My work focuses on making intelligent systems *predictable, auditable, and stable over time*, even as models change.

I publish in public, prototype aggressively, and turn promising ideas into reference implementations with clear specifications and testable guarantees.

---

## ğŸ”­ What I work on
- Constraint-first AI architectures (alignment as projection, not training)
- Deterministic governance layers for LLMs and agents
- Memory, identity, and safety enforced via external invariants
- Research systems that are falsifiable, auditable, and model-agnostic

---

## ğŸ› ï¸ Core skills & tools
- **Languages:** Python, TypeScript, Rust  
- **ML / Research:** PyTorch, JAX, scikit-learn, embedding & NLI models  
- **Systems:** Docker, CI/CD, cloud infra, reproducible experiments  
- **Design:** Constraint programming, optimization, safety & alignment  
- **Practice:** Testing, receipts/audit logs, spec-driven development

---

## ğŸš€ Featured project

### ğŸ”¹ Constraint-First-Intelligence (Î _Projector)  
**Repository:** [Constraint-First-Intelligence](https://github.com/robertwmccarty-caelum/Constraint-First-Intelligence)

A **model-agnostic governance architecture** that treats AI alignment as a geometric projection problem rather than a training objective.

Key ideas:
- Alignment as projection over immutable invariants
- Minimal Unsatisfiable Sets (MUS) for explainable refusals
- Deterministic, monotonic repair
- Typed Clarification Requests instead of vague refusals
- Cryptographic memory & predicate receipts
- Measurable governance metrics (CRR, ATR, stability)

This repo serves as a **canonical reference implementation** for constraint-first intelligence systems.

---

## ğŸ“ˆ What youâ€™ll find in my repos
- Reference architectures and formal specs  
- Deterministic repair and governance code  
- Experiments that test stability across model swaps  
- Design docs, notebooks, and implementation notes  
- Code intended to be **read, audited, and extended**

---

## ğŸ¤ Collaboration
I enjoy collaborating with:
- AI safety & alignment researchers
- Systems and infra engineers
- Teams building production LLM tooling

Iâ€™m open to:
- Research collaboration
- Design reviews and technical advising
- Select consulting or contracting (architecture-level work)

---

## ğŸ“« Contact
- GitHub: [robertwmccarty-caelum](https://github.com/robertwmccarty-caelum)  
<!-- Provide any of the below and Iâ€™ll add them into the README -->
- Email: your-email@example.com  
- Website / Portfolio: https://your-website.example  
- X / Twitter: https://twitter.com/your-handle  
- LinkedIn: https://www.linkedin.com/in/your-profile  
- Mastodon: https://mastodon.social/@your-handle

---

## ğŸ§  Philosophy
> Capability can be learned.  
> **Continuity must be enforced.**

Iâ€™m interested in systems that donâ€™t just work *now*, but remain correct, explainable, and governable as they scale.

Thanks for stopping by